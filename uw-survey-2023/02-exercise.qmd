---
title: "Fitting a spatiotemporal model and deriving an abundance index"
format: html
editor: visual
execute: 
  echo: true
  eval: true
---

# Goals:

-   Practice fitting a basic spatiotemporal model.
-   Understand how to inspect the model output.
-   Practice predicting from the model on new data and making visualizations of those predictions.
-   Gain familiarity with fitting, comparing and interpreting different random field structures.
-   Calculate an area-weighted biomass index and compare how model structure can impact an index.

```{r, message=FALSE, warning=FALSE, echo = FALSE}
library(sdmTMB)
library(mgcv)
library(gratia)
library(visreg)
library(dplyr)
library(ggplot2)
library(purrr)
options(ggplot2.continuous.colour = "viridis")
options(ggplot2.continuous.fill = "viridis")
theme_set(theme_light())
```

# The data

We will work with data representing North Pacific Spiny Dogfish in the West Coast Vancouver Island synoptic trawl survey.

```{r}
dat <- readRDS(here::here("uw-survey-2023/data/wcvi-dogfish.rds"))
```

```{r}
head(dat)
```

The dataset contains sampling locations (`longitude` and `latitude`) and `year`. It also contains sampling `depth` in meters and sample density `density` (CPUE) in units of tonnes/km^2^.

```{r}
ggplot(dat, aes(longitude, latitude, size = density)) + geom_point()
```

# Adding UTMs

We can add UTM columns using `add_utm_columns()`:

```{r}
add_utm_columns(dat, ll_names = c("longitude", "latitude"))
```

Note that this function guesses at an appropriate UTM projection for you and provides a URL to verify. To ensure future compatibility with prediction grids or other data, it is best to hard code the choice using the `utm_crs` argument. We will use `CRS = 3156` here to match our prediction grid:

```{r}
dat <- add_utm_columns(dat, utm_crs = 3156, ll_names = c("longitude", "latitude"))
```

And check to make sure that looks right:

```{r}
ggplot(dat, aes(X, Y, size = density)) + geom_point(shape = 21) + coord_fixed()
```

We can also plot the data by year:

```{r}
ggplot(dat, aes(X, Y, size = density, colour = log(density + 1))) +
  geom_point(alpha = 0.3) +
  facet_wrap(~year) + coord_fixed()
```

We will create these new columns to use later:

```{r}
dat$log_depth <- log(dat$depth)
dat$year_factor <- as.factor(dat$year)
```

# Constructing a mesh

We start by constructing an SPDE mesh with INLA. This creates some matrices that are used internally when fitting the model. We will use the shortcut function `make_mesh()` and use a cutoff (minimum triangle length of 10 km). Note: this is the only parameter that can be changed in `make_mesh()` but more control over mesh arguments can be done with INLA (`inla.mesh.2d()`).

```{r}
mesh <- make_mesh(dat, xy_cols = c("X", "Y"), cutoff = 10)
plot(mesh)
mesh$mesh$n # number of vertices or knots

# ggplot alternative:
ggplot() + inlabru::gg(mesh$mesh) + coord_fixed() +
  geom_point(aes(X, Y), data = dat, alpha = 0.2, size = 0.5)
```

### Exercise:

1.  Try adjusting the size of the cutoff distance to explore the effects of decreasing the mesh resolution. Make sure to reset the `cutoff` value to `10` in the end so the rest of the exercise behaves as intended, because model convergence issues can be caused by meshes that are either too fine or too coarse.

# Fitting a spatial model

The most basic model we could fit would be a model with a single spatial random field, and no covariates. This is similar to what we were fitting yesterday. Using `silent = FALSE` lets us see what is happening, but it's awkward when running code in Rmd/Quarto chunks (with the default RStudio setting to 'Show output inline for all R Markdown document') so we will comment it out in most cases here. But it is a good idea to use it if models are running slowly or not converging to monitor progress.

```{r, results='hide', warning=FALSE}
fit_spatial <- sdmTMB(
  density ~ 1, # intercept only
  data = dat,  
  family = tweedie(link = "log"),
  mesh = mesh,
  spatial = "on",
  # silent = FALSE
)
```

```{r}
sanity(fit_spatial)
```

Did it have trouble? If so, try an extra optimization run and see if that's sufficient:

```{r}
fit_spatial <- run_extra_optimization(fit_spatial)
sanity(fit_spatial)
```

```{r}
fit_spatial
```

We won't dwell on this model here for the sake of time, but in practice we would stop, inspect, and understand this model before progressing to a spatiotemporal model.

# Fitting a model with spatial + spatiotemporal fields

This first model includes a quadratic effect of log depth (`poly(log_depth, 2)`), a factor effect for each year, and models total density using a Tweedie distribution and a log link. The spatial field and spatiotemporal fields are estimated.

The year factors give each year its own mean (this is generally the approach used in fisheries stock assessment). The `0 +` omits the intercept such that each year's estimate represents a mean as opposed to a difference from the intercept. This part is arbitrary and chosen for the sake of this exercise.

The choice to use log depth helps the model fit because we have fewer samples from the deeper depths.

```{r, results='hide'}
fit <- sdmTMB(
  density ~ 0 + year_factor + poly(log_depth, 2),
  data = dat,
  family = tweedie(link = "log"),
  mesh = mesh,
  spatial = "on",
  spatiotemporal = "iid", #< new
  time = "year", #< new
  # silent = FALSE
)
```

Print the model:

```{r}
fit
```

Run a basic sanity check on the model:

```{r}
sanity(fit)
```

There are multiple ways we can plot the depth effect on density. The first approach is to pass the sdmTMB object to the visreg package. This shows the conditional effect, where all values other than depth are held at a particular value. Note the default visreg plot is in link (here, log) space and the dots are randomized quantile residuals.

```{r}
visreg::visreg(fit, xvar = "log_depth")
visreg::visreg(fit, xvar = "log_depth", scale = "response")
```

Second, we could use the `ggeffects` package, which can be used to show the marginal effects of predictors (averaging over all other covariates rather than using a single fixed value). For more details see the [visualizing marginal effects vignette](https://pbs-assess.github.io/sdmTMB/articles/ggeffects.html). *Note that won't yet work with smoother `s()` terms*, but will soon work with the similar `ggeffects::ggpredict()` .

```{r}
g <- ggeffects::ggeffect(fit, "log_depth [3.5:6.7 by=0.05]")
plot(g)
```

# Prediction

Let's now predict on a grid that covers the entire survey (`wcvi_grid`).

```{r}
wcvi_grid <- readRDS(here::here("uw-survey-2023/data/wcvi-grid.rds"))
head(wcvi_grid)
wcvi_grid$log_depth <- log(wcvi_grid$depth)
```

We can use this fancy line of purrr code to expand our grid to every year we want to predict on:

```{r}
grid <- purrr::map_dfr(unique(dat$year), ~ tibble(wcvi_grid, year = .x))
grid$year_factor <- as.factor(grid$year)
head(grid)
```

We can predict on the original data:

```{r}
p0 <- predict(fit)
```

To predict on a new data frame, we can specify `newdata`. Here, we will predict on the survey grid. (1) This makes it easy to make visualizations. (2) This will be useful if we wanted to generate an area-weighted standardized population index later.

```{r}
p <- predict(fit, newdata = grid)
```

We can plot each of the components of the prediction data frame spatially:

```{r}
# Depth and year effect contribution:
# (Everything not a random field)
ggplot(p, aes(X, Y, fill = exp(est_non_rf))) +
  facet_wrap(~year) +
  geom_raster() +
  coord_fixed()

# Spatial random field:
ggplot(p, aes(X, Y, fill = omega_s)) +
  facet_wrap(~year) +
  geom_raster() +
  scale_fill_gradient2() +
  coord_fixed()

# Spatial-temporal random field:
ggplot(p, aes(X, Y, fill = epsilon_st)) +
  facet_wrap(~year) +
  geom_raster() +
  scale_fill_gradient2() +
  coord_fixed()

# Overall estimate of density in link (log) space:
ggplot(p, aes(X, Y, fill = est)) +
  facet_wrap(~year) +
  geom_raster() +
  coord_fixed()

# Overall estimate of density: (with log-distributed colour)
ggplot(p, aes(X, Y, fill = exp(est))) +
  facet_wrap(~year) +
  geom_raster() +
  coord_fixed() +
  scale_fill_viridis_c(trans = "log10")
```

# Residual checking

We can calculate randomized quantile residuals with the `residuals.sdmTMB()` function.

```{r}
dat$resid <- residuals(fit)
```

We can plot those residuals spatially:

```{r}
ggplot(dat, aes(X, Y, colour = resid)) +
  facet_wrap(~year) +
  geom_point(size = 0.5) +
  coord_fixed() +
  scale_colour_gradient2()
```

## Exercise:

1.  What are you looking for in the above? Does it look OK?

We can check the distribution of the residuals with a QQ plot:

```{r}
qqnorm(dat$resid)
qqline(dat$resid)
```

These don't look great, *but*, this is largely a function of error from the Laplace approximation on the random effects (see [Thygesen et al. 2017](https://doi.org/10.1007/s10651-017-0372-4)). MCMC-based are a better approach, but are slower. There's a whole vignette on residual checking with sdmTMB here: <https://pbs-assess.github.io/sdmTMB/articles/residual-checking.html>

# Fitting an anisotropic model

We will re-fit our model with REML so we can use AIC to compare models with different random effect structures (although not strictly necessary). The model that we fit above assumes isotropy, where correlation decays in all directions at same rate (this is the default behavior). We will compare this fit of the isotropic model to a model assuming anisotropy, or directionally dependent spatial correlation.

We will use the handy method `update()` here, but we could also re-write the whole `sdmTMB()` function call each time.

```{r, results='hide'}
fit_reml <- update(fit, reml = TRUE)
fit_aniso <- update(fit, reml = TRUE, anisotropy = TRUE)
```

```{r}
sanity(fit_reml)
sanity(fit_aniso)
```

```{r}
fit_reml
fit_aniso
```

Anisotropic range is best examined graphically:

```{r}
plot_anisotropy(fit_aniso)
```

Notice how the correlation occurs over longer distances along the NW/SE plane than the NE/SW plane. In contrast, the correlation distance is constant in all dimensions in our initial (isotropic) model. In other words, if we were to make a similar plot to the above for the isotropic model, it would be a circle whose radius is the estimated spatial range.

We can check the AIC of the 2 models:

```{r}
AIC(fit_reml, fit_aniso)
```

Note that anisotropy is often important to include when predicting to regions with a strong directional gradient in a (modeled or latent) habitat covariate, such as depth along a narrow continental shelf.

## Exercise:

1.  What does AIC suggest? (Keep in mind AIC is a bit suspect for these kinds of models with correlated random effects, often selects more complicated models, and shouldn't be taken *too* seriously.) Generally, cross-validation and other methods are better for model comparison of mixed effect models, but that is beyond the scope of this lesson. For more information, see this vignette on cross-validation: <https://pbs-assess.github.io/sdmTMB/articles/web_only/cross-validation.html>

# Index standardization

To calculate an index from any of these models, we need to run the `predict.sdmTMB()` function with the argument `return_tmb_object = TRUE`. We can then run the `get_index()` function to extract the total biomass calculations and standard errors.

We can set the area argument to our `cell_area` column in km^2^. In this case the value is 4 km^2^ for all of the cells, since our grid cells are 2 km x 2 km. If some grid cells were not fully in the survey domain (or were on land), we could feed a vector of grid areas to the area argument that matched the number of grid cells. Because the density units are tonnes per km^2^ for this data, the index is in tonnes.

```{r}
p <- predict(fit, newdata = grid, return_tmb_object = TRUE)
index <- get_index(p, area = grid$cell_area, bias_correct = FALSE)

ggplot(index, aes(year, est)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  xlab("Year") +
  ylab("Biomass estimate (tonnes)")
```

We used `bias_correction = FALSE` to speed things up, but for any final result you will want to use the bias correction. Let's see how much the scale of the index changes with bias correction.

```{r}
index_c <- get_index(p, area = grid$cell_area, bias_correct = TRUE)
index_c$Method <- "Bias correction"

bind_rows(index, index_c) %>%
  ggplot(aes(year, est, fill = Method)) +
  geom_line(aes(colour = Method)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  xlab("Year") +
  ylab("Biomass estimate (tonnes)")
```

# Fit a few GAMs and estimate an abundance index

Include a covariate (depth)

Analogous to nonspatial model (spatial = "off", spatiotemporal = "off")

```{r}
fit_gam <- gam(
  formula = density ~ s(depth) + as.factor(year),
  family = tw(link = "log"),
  data = dat
)
```

Get model summary

```{r}
summary(fit_gam)
```

Plot partial effects of covariates

```{r}
plot(fit_gam, shade = TRUE, residuals = TRUE)
```

Get diagnostics and perform model checking

```{r}
gam.check(fit_gam)
```

Review console output to help verify convergence, and whether there were an adequate number of basis functions (k).

Examine the four diagnostic plots. Each of these gives a different way of looking at your model residuals. On the top-left is a Q-Q plot, which compares the model residuals to the expected/assumed distribution family. A well-fit model's residuals will be close to the 1-1 line, otherwise there may be under- or over-dispersion present. On bottom left is a histogram of residuals. We want this to have a shape similar to the distribution family we specified. On top-right is a plot of residual values as a function of the linear predictor. These should be evenly distributed around zero in a well-fitted model. Finally, on the bottom-right is plot of response against fitted values. A well-fitted model would show values near the 1-1 line.

# Predict to survey area (new data)

```{r}
pred_gam <- predict(fit_gam, type = "response", newdata = grid)
pred_gam_df <- cbind(grid, pred_gam)
```

Plot predictions over survey area

```{r}
ggplot(pred_gam_df, aes(X, Y, fill = pred_gam)) + geom_raster() +
  scale_fill_viridis_c() + facet_wrap(~year) + coord_fixed() +
  labs(fill = "Log Biomass density\n(kg/km^2)")
```

# Calculate biomass index from GAM via simulation

```{r}
sims <- gratia::fitted_samples(fit_gam, n=10, newdata=grid, 
                               scale="response", seed=9)
sims$year <- grid$year[sims$row]
sims$biomass <- sims$fitted * 4 # expand from density to biomass, given area

level <- 0.95 # specify probability for confidence interval

# Get sum of simulated biomass (density*area) across grid cells, with CI
lwr_fn <- function(x) {as.numeric(quantile(x, probs = (1 - level) / 2))}
upr_fn <- function(x) {as.numeric(quantile(x, probs = 1 - (1 - level) / 2))}

sims_sum <-  sims %>% 
  group_by(year,draw) %>% 
  summarise_at("biomass", list(biomass = sum)) %>%
  group_by(year) %>%
  summarise_at("biomass", list(est = median, # could use mean
                           lwr = lwr_fn,
                           upr = upr_fn))
```

Note that this approach uses a Gaussian approximation to the posterior, which is all that is implemented currently in the gratia package. However, a better estimate of uncertainty could be derived from sampling from the actual posterior distribution. However, this is beyond the scope of today's lesson.

Plot the biomass index

```{r}
ggplot(sims_sum, aes(year, est)) + geom_line() +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.4) +
  xlab('Year') + ylab('Biomass estimate (kg)')
```

# Fit a GAM analogous to a spatial model in sdmTMB

Include a 2-D smooth over space

```{r}
fit_gam_s <- gam(
  formula = density ~ s(depth) + as.factor(year) + 
    s(X,Y), #<<
  family = tw(link = "log"),
  data = dat
)
```

Visualize smooths, where the first plot shows the 1-D smooth on depth and the second plot shows the 2-D (or bivariate) spatial smooth

```{r}
plot(fit_gam_s)
```

or

```{r}
plot(fit_gam_s, scheme = 1)
```

or

```{r}
plot(fit_gam_s, scheme = 2)
```

Note that you may have to play around with these types of visualizations because 2-D smooths can be tough to visualize. Also see the vis.gam package for more options.

# Fit a GAM analogous to a spatiotemporal model in sdmTMB

Include a 2-D smooth over space for each year

```{r}
fit_gam_st <- gam(
  formula = density ~ s(depth) + as.factor(year) +
    s(X,Y, by = year), #<<
  family = tw(link = "log"),
  data = dat)
```

### Exercise:

1.  Try fitting similar models and estimating indices from your own data. Do a lot of data visualization and consider different distribution families (e.g. Gaussian).

Don't hesitate to ask questions!
